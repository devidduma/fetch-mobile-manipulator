G:\DD\fetch-mobile-manipulator\venv\Scripts\python.exe G:\DD\fetch-mobile-manipulator\examples\fetch_her_td3.py --task FetchReach-v3 --hidden-sizes 256 256 --actor-lr 1e-3 --critic-lr 3e-3 --gamma 0.98 --epoch 10 --step-per-epoch 5000
Observations shape: {'observation': (10,), 'achieved_goal': (3,), 'desired_goal': (3,)}
Actions shape: (4,)
Action range: -1.0 1.0
Epoch #1: 5001it [03:02, 27.46it/s, env_step=5000, len=50, loss/actor=3.305, loss/critic1=0.380, loss/critic2=0.339, n/ep=1, n/st=1, rew=0.00]
Epoch #1: test_reward: -1.900000 ± 0.538516, best_reward: -1.900000 ± 0.538516 in #1
Epoch #2: 5001it [02:53, 28.76it/s, env_step=10000, len=50, loss/actor=1.068, loss/critic1=0.136, loss/critic2=0.128, n/ep=1, n/st=1, rew=-1.00]
Epoch #3:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #2: test_reward: -1.900000 ± 1.044031, best_reward: -1.900000 ± 0.538516 in #1
Epoch #3: 5001it [03:02, 27.44it/s, env_step=15000, len=50, loss/actor=0.641, loss/critic1=0.091, loss/critic2=0.098, n/ep=1, n/st=1, rew=-1.00]
Epoch #3: test_reward: -1.600000 ± 0.663325, best_reward: -1.600000 ± 0.663325 in #3
Epoch #4: 5001it [03:06, 26.84it/s, env_step=20000, len=50, loss/actor=0.459, loss/critic1=0.070, loss/critic2=0.077, n/ep=1, n/st=1, rew=-2.00]
Epoch #5:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #4: test_reward: -1.500000 ± 0.806226, best_reward: -1.500000 ± 0.806226 in #4
Epoch #5: 5001it [03:10, 26.22it/s, env_step=25000, len=50, loss/actor=0.441, loss/critic1=0.073, loss/critic2=0.073, n/ep=1, n/st=1, rew=-2.00]
Epoch #6:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #5: test_reward: -1.300000 ± 0.640312, best_reward: -1.300000 ± 0.640312 in #5
Epoch #6: 5001it [03:11, 26.06it/s, env_step=30000, len=50, loss/actor=0.474, loss/critic1=0.057, loss/critic2=0.056, n/ep=1, n/st=1, rew=-2.00]
Epoch #6: test_reward: -1.900000 ± 1.044031, best_reward: -1.300000 ± 0.640312 in #5
Epoch #7: 5001it [03:14, 25.67it/s, env_step=35000, len=50, loss/actor=0.509, loss/critic1=0.058, loss/critic2=0.059, n/ep=1, n/st=1, rew=-3.00]
Epoch #7: test_reward: -1.800000 ± 0.871780, best_reward: -1.300000 ± 0.640312 in #5
Epoch #8: 5001it [03:14, 25.66it/s, env_step=40000, len=50, loss/actor=0.502, loss/critic1=0.055, loss/critic2=0.052, n/ep=1, n/st=1, rew=-1.00]
Epoch #8: test_reward: -1.600000 ± 0.916515, best_reward: -1.300000 ± 0.640312 in #5
Epoch #9: 5001it [03:16, 25.40it/s, env_step=45000, len=50, loss/actor=0.470, loss/critic1=0.051, loss/critic2=0.049, n/ep=1, n/st=1, rew=-2.00]
Epoch #10:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #9: test_reward: -2.000000 ± 0.894427, best_reward: -1.300000 ± 0.640312 in #5
Epoch #10: 5001it [03:16, 25.47it/s, env_step=50000, len=50, loss/actor=0.438, loss/critic1=0.044, loss/critic2=0.043, n/ep=1, n/st=1, rew=-3.00]
Epoch #10: test_reward: -1.700000 ± 1.100000, best_reward: -1.300000 ± 0.640312 in #5
{'best_result': '-1.30 ± 0.64',
 'best_reward': -1.3,
 'duration': '1893.49s',
 'test_episode': 110,
 'test_speed': '1712.86 step/s',
 'test_step': 5500,
 'test_time': '3.21s',
 'train_episode': 1000,
 'train_speed': '26.45 step/s',
 'train_step': 50000,
 'train_time/collector': '366.18s',
 'train_time/model': '1524.09s'}
Final reward: -2.2, length: 50.0

Process finished with exit code 0
