{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IO0ADjYzh7T1",
    "outputId": "75327f54-f8f1-4917-d76c-507303052cc8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfuzeq7BitX9",
    "outputId": "faa4d8cb-9e97-43c4-eb2f-c3c107e59125"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Markus28/tianshou.git@gymnasium_integration#egg=tianshou --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyOq9We-I6ay",
    "outputId": "ada6290d-22e6-4cdf-d369-60ede5420445"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "libglew-dev is already the newest version (2.1.0-4).\n",
      "libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "libgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "libosmesa6-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "software-properties-common is already the newest version (0.99.9.10).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-510\n",
      "Use 'apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "patchelf is already the newest version (0.10-2build1).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-510\n",
      "Use 'apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get -q install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get -q install -y patchelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UbOlVWv0157i"
   },
   "outputs": [],
   "source": [
    "!pip install envpool wandb pettingzoo gymnasium-robotics --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fCkkosakI_bC"
   },
   "outputs": [],
   "source": [
    "!pip install free-mujoco-py --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7TZHDrP1JTUj"
   },
   "outputs": [],
   "source": [
    "import mujoco_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jtbz6g5yj3FL",
    "outputId": "09bb8356-3bdd-4a81-da76-43b00a711ff6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2023-02-23 14:28:47.020277: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin\n",
      "2023-02-23 14:28:47.020566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin\n",
      "2023-02-23 14:28:47.020601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Observations shape: {'observation': (25,), 'achieved_goal': (3,), 'desired_goal': (3,)}\n",
      "Actions shape: (4,)\n",
      "Action range: -1.0 1.0\n",
      "Epoch #1: 5001it [09:17,  8.97it/s, env_step=5000, len=50, loss/actor=1.281, loss/critic1=0.158, loss/critic2=0.159, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #1: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #0\n",
      "Epoch #2: 5001it [09:21,  8.91it/s, env_step=10000, len=50, loss/actor=1.936, loss/critic1=0.378, loss/critic2=0.382, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #2: test_reward: -45.000000 ± 15.000000, best_reward: -40.000000 ± 20.000000 in #0\n",
      "Epoch #3: 5001it [09:35,  8.70it/s, env_step=15000, len=50, loss/actor=2.448, loss/critic1=0.628, loss/critic2=0.628, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #3: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #0\n",
      "Epoch #4: 5001it [09:42,  8.59it/s, env_step=20000, len=50, loss/actor=2.231, loss/critic1=0.703, loss/critic2=0.704, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #4: test_reward: -45.000000 ± 15.000000, best_reward: -40.000000 ± 20.000000 in #0\n",
      "Epoch #5: 5001it [09:57,  8.37it/s, env_step=25000, len=50, loss/actor=2.853, loss/critic1=0.963, loss/critic2=0.958, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #5: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #0\n",
      "Epoch #6: 5001it [09:53,  8.42it/s, env_step=30000, len=50, loss/actor=2.922, loss/critic1=1.189, loss/critic2=1.195, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #6: test_reward: -35.000000 ± 22.912878, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #7: 5001it [09:58,  8.35it/s, env_step=35000, len=50, loss/actor=2.270, loss/critic1=1.460, loss/critic2=1.460, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #7: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #8: 5001it [10:03,  8.29it/s, env_step=40000, len=50, loss/actor=2.041, loss/critic1=1.269, loss/critic2=1.271, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #8: test_reward: -35.000000 ± 22.912878, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #9: 5001it [10:13,  8.16it/s, env_step=45000, len=50, loss/actor=2.490, loss/critic1=1.228, loss/critic2=1.234, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #9: test_reward: -44.800000 ± 14.012851, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #10: 5001it [10:17,  8.09it/s, env_step=50000, len=50, loss/actor=2.462, loss/critic1=1.134, loss/critic2=1.130, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #10: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #11: 5001it [10:22,  8.04it/s, env_step=55000, len=50, loss/actor=2.531, loss/critic1=1.267, loss/critic2=1.259, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #11: test_reward: -40.000000 ± 20.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #12: 5001it [10:25,  8.00it/s, env_step=60000, len=50, loss/actor=2.253, loss/critic1=1.026, loss/critic2=1.022, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #12: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #13: 5001it [10:30,  7.93it/s, env_step=65000, len=50, loss/actor=2.784, loss/critic1=1.319, loss/critic2=1.313, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #13: test_reward: -49.500000 ± 1.500000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #14: 5001it [10:33,  7.90it/s, env_step=70000, len=50, loss/actor=3.013, loss/critic1=1.238, loss/critic2=1.227, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #14: test_reward: -44.100000 ± 14.942891, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #15: 5001it [10:49,  7.70it/s, env_step=75000, len=50, loss/actor=2.954, loss/critic1=1.313, loss/critic2=1.293, n/ep=1, n/st=1, rew=0.00]              \n",
      "Epoch #15: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #16: 5001it [11:52,  7.02it/s, env_step=80000, len=50, loss/actor=3.047, loss/critic1=1.327, loss/critic2=1.330, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #16: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #17: 5001it [11:48,  7.06it/s, env_step=85000, len=50, loss/actor=3.082, loss/critic1=1.432, loss/critic2=1.433, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #17: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #18: 5001it [11:52,  7.01it/s, env_step=90000, len=50, loss/actor=3.196, loss/critic1=1.484, loss/critic2=1.482, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #18: test_reward: -40.000000 ± 20.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #19: 5001it [11:52,  7.02it/s, env_step=95000, len=50, loss/actor=3.291, loss/critic1=1.445, loss/critic2=1.436, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #19: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #20: 5001it [11:14,  7.42it/s, env_step=100000, len=50, loss/actor=3.513, loss/critic1=1.706, loss/critic2=1.685, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #20: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #21: 5001it [10:55,  7.62it/s, env_step=105000, len=50, loss/actor=3.516, loss/critic1=1.540, loss/critic2=1.523, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #21: test_reward: -40.000000 ± 20.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #22: 5001it [10:51,  7.67it/s, env_step=110000, len=50, loss/actor=3.748, loss/critic1=1.612, loss/critic2=1.609, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #22: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #23: 5001it [10:54,  7.64it/s, env_step=115000, len=50, loss/actor=4.216, loss/critic1=1.724, loss/critic2=1.722, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #23: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #24: 5001it [10:56,  7.62it/s, env_step=120000, len=50, loss/actor=3.961, loss/critic1=1.522, loss/critic2=1.508, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #24: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #25: 5001it [10:59,  7.59it/s, env_step=125000, len=50, loss/actor=3.572, loss/critic1=1.822, loss/critic2=1.818, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #25: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #26: 5001it [10:58,  7.59it/s, env_step=130000, len=50, loss/actor=3.007, loss/critic1=1.249, loss/critic2=1.245, n/ep=1, n/st=1, rew=0.00]\n",
      "Epoch #26: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #27: 5001it [11:01,  7.57it/s, env_step=135000, len=50, loss/actor=2.738, loss/critic1=1.409, loss/critic2=1.401, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #27: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #28: 5001it [11:10,  7.46it/s, env_step=140000, len=50, loss/actor=2.701, loss/critic1=1.344, loss/critic2=1.340, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #28: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #29: 5001it [11:07,  7.50it/s, env_step=145000, len=50, loss/actor=3.205, loss/critic1=1.553, loss/critic2=1.537, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #29: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #30: 5001it [11:09,  7.47it/s, env_step=150000, len=50, loss/actor=2.737, loss/critic1=1.542, loss/critic2=1.527, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #30: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #31: 5001it [11:09,  7.47it/s, env_step=155000, len=50, loss/actor=2.580, loss/critic1=1.528, loss/critic2=1.514, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #31: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #32: 5001it [11:12,  7.44it/s, env_step=160000, len=50, loss/actor=2.673, loss/critic1=1.566, loss/critic2=1.545, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #32: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #33: 5001it [11:03,  7.54it/s, env_step=165000, len=50, loss/actor=3.384, loss/critic1=1.812, loss/critic2=1.790, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #33: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #34: 5001it [11:05,  7.52it/s, env_step=170000, len=50, loss/actor=2.674, loss/critic1=1.439, loss/critic2=1.428, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #34: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #35: 5001it [11:03,  7.54it/s, env_step=175000, len=50, loss/actor=2.697, loss/critic1=1.171, loss/critic2=1.161, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #35: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #36: 5001it [11:08,  7.48it/s, env_step=180000, len=50, loss/actor=2.675, loss/critic1=1.313, loss/critic2=1.310, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #36: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #37: 5001it [11:06,  7.51it/s, env_step=185000, len=50, loss/actor=2.804, loss/critic1=1.288, loss/critic2=1.279, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #37: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #38: 5001it [11:09,  7.47it/s, env_step=190000, len=50, loss/actor=3.056, loss/critic1=1.450, loss/critic2=1.446, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #38: test_reward: -42.200000 ± 16.357261, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #39: 5001it [11:04,  7.52it/s, env_step=195000, len=50, loss/actor=3.147, loss/critic1=1.561, loss/critic2=1.563, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #39: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #40: 5001it [11:17,  7.39it/s, env_step=200000, len=50, loss/actor=3.121, loss/critic1=1.286, loss/critic2=1.282, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #40: test_reward: -49.700000 ± 0.900000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #41: 5001it [11:10,  7.46it/s, env_step=205000, len=50, loss/actor=2.979, loss/critic1=1.415, loss/critic2=1.413, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #41: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #42: 5001it [11:33,  7.21it/s, env_step=210000, len=50, loss/actor=2.822, loss/critic1=1.203, loss/critic2=1.202, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #42: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #43: 5001it [11:59,  6.95it/s, env_step=215000, len=50, loss/actor=2.975, loss/critic1=1.238, loss/critic2=1.220, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #43: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #44: 5001it [11:55,  6.99it/s, env_step=220000, len=50, loss/actor=3.210, loss/critic1=1.256, loss/critic2=1.250, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #44: test_reward: -40.000000 ± 20.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #45: 5001it [11:34,  7.20it/s, env_step=225000, len=50, loss/actor=3.335, loss/critic1=1.133, loss/critic2=1.133, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #45: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #46: 5001it [11:22,  7.33it/s, env_step=230000, len=50, loss/actor=3.422, loss/critic1=1.354, loss/critic2=1.338, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #46: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #47: 5001it [11:28,  7.26it/s, env_step=235000, len=50, loss/actor=3.331, loss/critic1=1.160, loss/critic2=1.149, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #47: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #48: 5001it [11:28,  7.26it/s, env_step=240000, len=50, loss/actor=3.397, loss/critic1=1.236, loss/critic2=1.232, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #48: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #49: 5001it [11:23,  7.32it/s, env_step=245000, len=50, loss/actor=3.321, loss/critic1=1.271, loss/critic2=1.263, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #49: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #50: 5001it [11:13,  7.42it/s, env_step=250000, len=50, loss/actor=3.109, loss/critic1=1.190, loss/critic2=1.185, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #50: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #51: 5001it [11:23,  7.32it/s, env_step=255000, len=50, loss/actor=2.996, loss/critic1=1.177, loss/critic2=1.156, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #51: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #52: 5001it [11:18,  7.37it/s, env_step=260000, len=50, loss/actor=3.066, loss/critic1=1.108, loss/critic2=1.103, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #52: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #53: 5001it [11:28,  7.27it/s, env_step=265000, len=50, loss/actor=2.970, loss/critic1=1.278, loss/critic2=1.280, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #53: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #54: 5001it [11:31,  7.23it/s, env_step=270000, len=50, loss/actor=2.923, loss/critic1=1.271, loss/critic2=1.260, n/ep=1, n/st=1, rew=0.00]\n",
      "Epoch #54: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #55: 5001it [11:51,  7.03it/s, env_step=275000, len=50, loss/actor=2.896, loss/critic1=1.207, loss/critic2=1.202, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #55: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #56: 5001it [12:16,  6.79it/s, env_step=280000, len=50, loss/actor=2.755, loss/critic1=1.288, loss/critic2=1.295, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #56: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #57: 5001it [12:28,  6.68it/s, env_step=285000, len=50, loss/actor=2.728, loss/critic1=1.150, loss/critic2=1.146, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #57: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #58: 5001it [11:57,  6.97it/s, env_step=290000, len=50, loss/actor=2.678, loss/critic1=1.264, loss/critic2=1.241, n/ep=1, n/st=1, rew=0.00]\n",
      "Epoch #58: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #59: 5001it [11:59,  6.95it/s, env_step=295000, len=50, loss/actor=2.769, loss/critic1=1.134, loss/critic2=1.132, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #59: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #60: 5001it [12:08,  6.86it/s, env_step=300000, len=50, loss/actor=2.731, loss/critic1=0.952, loss/critic2=0.943, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #60: test_reward: -40.000000 ± 20.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #61: 5001it [12:08,  6.86it/s, env_step=305000, len=50, loss/actor=2.674, loss/critic1=1.023, loss/critic2=1.014, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #61: test_reward: -40.000000 ± 20.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #62: 5001it [12:04,  6.90it/s, env_step=310000, len=50, loss/actor=2.724, loss/critic1=0.970, loss/critic2=0.971, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #62: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #63: 5001it [12:17,  6.78it/s, env_step=315000, len=50, loss/actor=2.821, loss/critic1=1.212, loss/critic2=1.204, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #63: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #64: 5001it [12:08,  6.86it/s, env_step=320000, len=50, loss/actor=2.846, loss/critic1=1.263, loss/critic2=1.251, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #64: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #65: 5001it [12:21,  6.75it/s, env_step=325000, len=50, loss/actor=2.956, loss/critic1=1.100, loss/critic2=1.093, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #65: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #66: 5001it [12:06,  6.88it/s, env_step=330000, len=50, loss/actor=2.941, loss/critic1=1.128, loss/critic2=1.121, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #66: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #67: 5001it [11:58,  6.96it/s, env_step=335000, len=50, loss/actor=2.934, loss/critic1=1.040, loss/critic2=1.037, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #67: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #68: 5001it [11:34,  7.20it/s, env_step=340000, len=50, loss/actor=3.009, loss/critic1=0.992, loss/critic2=0.988, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #68: test_reward: -40.000000 ± 20.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #69: 5001it [11:29,  7.26it/s, env_step=345000, len=50, loss/actor=3.275, loss/critic1=1.191, loss/critic2=1.192, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #69: test_reward: -49.700000 ± 0.900000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #70: 5001it [11:24,  7.30it/s, env_step=350000, len=50, loss/actor=2.954, loss/critic1=1.141, loss/critic2=1.131, n/ep=1, n/st=1, rew=0.00]              \n",
      "Epoch #70: test_reward: -44.800000 ± 14.945233, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #71: 5001it [11:14,  7.41it/s, env_step=355000, len=50, loss/actor=2.892, loss/critic1=0.944, loss/critic2=0.934, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #71: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #72: 5001it [11:13,  7.43it/s, env_step=360000, len=50, loss/actor=2.869, loss/critic1=0.993, loss/critic2=0.981, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #72: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #73: 5001it [11:15,  7.40it/s, env_step=365000, len=50, loss/actor=2.950, loss/critic1=0.949, loss/critic2=0.930, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #73: test_reward: -44.500000 ± 14.908052, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #74: 5001it [11:10,  7.46it/s, env_step=370000, len=50, loss/actor=2.944, loss/critic1=1.167, loss/critic2=1.169, n/ep=1, n/st=1, rew=0.00]              \n",
      "Epoch #74: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #75: 5001it [11:13,  7.43it/s, env_step=375000, len=50, loss/actor=2.822, loss/critic1=1.116, loss/critic2=1.108, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #75: test_reward: -44.500000 ± 14.908052, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #76: 5001it [11:11,  7.45it/s, env_step=380000, len=50, loss/actor=3.097, loss/critic1=1.087, loss/critic2=1.079, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #76: test_reward: -44.500000 ± 14.908052, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #77: 5001it [11:07,  7.49it/s, env_step=385000, len=50, loss/actor=3.052, loss/critic1=1.184, loss/critic2=1.168, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #77: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #78: 5001it [11:12,  7.43it/s, env_step=390000, len=50, loss/actor=2.987, loss/critic1=1.077, loss/critic2=1.077, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #78: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #79: 5001it [11:10,  7.46it/s, env_step=395000, len=50, loss/actor=3.026, loss/critic1=1.147, loss/critic2=1.128, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #79: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #80: 5001it [11:17,  7.38it/s, env_step=400000, len=50, loss/actor=3.343, loss/critic1=1.223, loss/critic2=1.212, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #80: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #81: 5001it [11:07,  7.49it/s, env_step=405000, len=50, loss/actor=3.498, loss/critic1=1.143, loss/critic2=1.133, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #81: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #82: 5001it [11:11,  7.44it/s, env_step=410000, len=50, loss/actor=3.548, loss/critic1=1.251, loss/critic2=1.246, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #82: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #83: 5001it [11:07,  7.49it/s, env_step=415000, len=50, loss/actor=3.733, loss/critic1=1.209, loss/critic2=1.201, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #83: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #84: 5001it [11:04,  7.52it/s, env_step=420000, len=50, loss/actor=3.657, loss/critic1=1.225, loss/critic2=1.216, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #84: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #85: 5001it [11:09,  7.47it/s, env_step=425000, len=50, loss/actor=3.444, loss/critic1=0.969, loss/critic2=0.954, n/ep=1, n/st=1, rew=0.00]\n",
      "Epoch #85: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #86: 5001it [10:56,  7.62it/s, env_step=430000, len=50, loss/actor=3.335, loss/critic1=0.994, loss/critic2=0.990, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #86: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #87: 5001it [10:56,  7.62it/s, env_step=435000, len=50, loss/actor=3.356, loss/critic1=1.075, loss/critic2=1.049, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #87: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #88: 5001it [10:56,  7.62it/s, env_step=440000, len=50, loss/actor=3.597, loss/critic1=1.234, loss/critic2=1.210, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #88: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #89: 5001it [11:02,  7.54it/s, env_step=445000, len=50, loss/actor=3.487, loss/critic1=1.159, loss/critic2=1.152, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #89: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #90: 5001it [11:09,  7.47it/s, env_step=450000, len=50, loss/actor=3.567, loss/critic1=1.373, loss/critic2=1.355, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #90: test_reward: -45.000000 ± 15.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #91: 5001it [11:14,  7.41it/s, env_step=455000, len=50, loss/actor=3.535, loss/critic1=1.108, loss/critic2=1.097, n/ep=1, n/st=1, rew=0.00]\n",
      "Epoch #91: test_reward: -44.200000 ± 14.878172, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #92: 5001it [10:59,  7.58it/s, env_step=460000, len=50, loss/actor=3.586, loss/critic1=1.095, loss/critic2=1.083, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #92: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #93: 5001it [11:02,  7.55it/s, env_step=465000, len=50, loss/actor=3.443, loss/critic1=1.306, loss/critic2=1.284, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #93: test_reward: -50.000000 ± 0.000000, best_reward: -35.000000 ± 22.912878 in #6\n",
      "Epoch #94: 5001it [10:57,  7.60it/s, env_step=470000, len=50, loss/actor=3.505, loss/critic1=1.044, loss/critic2=1.030, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #94: test_reward: -31.300000 ± 23.177791, best_reward: -31.300000 ± 23.177791 in #94\n",
      "Epoch #95: 5001it [11:01,  7.56it/s, env_step=475000, len=50, loss/actor=3.317, loss/critic1=1.269, loss/critic2=1.248, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #95: test_reward: -37.100000 ± 20.437466, best_reward: -31.300000 ± 23.177791 in #94\n",
      "Epoch #96: 5001it [10:52,  7.66it/s, env_step=480000, len=50, loss/actor=2.895, loss/critic1=1.189, loss/critic2=1.160, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #96: test_reward: -45.000000 ± 15.000000, best_reward: -31.300000 ± 23.177791 in #94\n",
      "Epoch #97: 5001it [10:52,  7.67it/s, env_step=485000, len=50, loss/actor=2.720, loss/critic1=0.993, loss/critic2=0.972, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #97: test_reward: -50.000000 ± 0.000000, best_reward: -31.300000 ± 23.177791 in #94\n",
      "Epoch #98: 5001it [10:56,  7.62it/s, env_step=490000, len=50, loss/actor=2.835, loss/critic1=1.094, loss/critic2=1.086, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #98: test_reward: -40.700000 ± 18.665744, best_reward: -31.300000 ± 23.177791 in #94\n",
      "Epoch #99: 5001it [10:57,  7.61it/s, env_step=495000, len=50, loss/actor=3.143, loss/critic1=1.301, loss/critic2=1.271, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #99: test_reward: -48.000000 ± 6.000000, best_reward: -31.300000 ± 23.177791 in #94\n",
      "Epoch #100: 5001it [10:52,  7.66it/s, env_step=500000, len=50, loss/actor=3.235, loss/critic1=1.174, loss/critic2=1.146, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #100: test_reward: -50.000000 ± 0.000000, best_reward: -31.300000 ± 23.177791 in #94\n",
      "{'best_result': '-31.30 ± 23.18',\n",
      " 'best_reward': -31.3,\n",
      " 'duration': '67230.37s',\n",
      " 'test_episode': 1010,\n",
      " 'test_speed': '235.71 step/s',\n",
      " 'test_step': 50500,\n",
      " 'test_time': '214.25s',\n",
      " 'train_episode': 10000,\n",
      " 'train_speed': '7.46 step/s',\n",
      " 'train_step': 500000,\n",
      " 'train_time/collector': '7971.28s',\n",
      " 'train_time/model': '59044.84s'}\n",
      "Final reward: -45.4, length: 50.0\n"
     ]
    }
   ],
   "source": [
    "!python fetch_her_td3.py --task FetchPush-v2 \\\n",
    "      --hidden-sizes 256 256 256 --actor-lr 1e-4 --critic-lr 1e-4 \\\n",
    "      --gamma 0.98 --epoch 100 --her-horizon 50"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
