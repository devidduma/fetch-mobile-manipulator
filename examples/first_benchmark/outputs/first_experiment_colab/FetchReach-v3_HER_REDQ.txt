2023-03-03 13:37:55.881039: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin
2023-03-03 13:37:55.881164: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin
2023-03-03 13:37:55.881184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Observations shape: {'observation': (10,), 'achieved_goal': (3,), 'desired_goal': (3,)}
Actions shape: (4,)
Action range: -1.0 1.0
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
Epoch #1: 5001it [03:10, 26.30it/s, env_step=5000, len=50, loss/actor=-0.544, loss/critics=0.288, n/ep=1, n/st=1, rew=-4.00]
Epoch #1: test_reward: -2.200000 ± 1.077033, best_reward: -2.200000 ± 1.077033 in #1
Epoch #2: 5001it [03:16, 25.42it/s, env_step=10000, len=50, loss/actor=-3.479, loss/critics=0.542, n/ep=1, n/st=1, rew=-7.00]
Epoch #2: test_reward: -2.400000 ± 1.019804, best_reward: -2.200000 ± 1.077033 in #1
Epoch #3: 5001it [03:24, 24.40it/s, env_step=15000, len=50, loss/actor=-5.056, loss/critics=0.856, n/ep=1, n/st=1, rew=-6.00]
Epoch #3: test_reward: -2.500000 ± 0.670820, best_reward: -2.200000 ± 1.077033 in #1
Epoch #4: 5001it [03:30, 23.81it/s, env_step=20000, len=50, loss/actor=-6.027, loss/critics=0.987, n/ep=1, n/st=1, rew=-7.00]
Epoch #4: test_reward: -2.000000 ± 0.774597, best_reward: -2.000000 ± 0.774597 in #4
Epoch #5: 5001it [03:34, 23.34it/s, env_step=25000, len=50, loss/actor=-6.665, loss/critics=1.256, n/ep=1, n/st=1, rew=-9.00]
Epoch #5: test_reward: -2.000000 ± 0.774597, best_reward: -2.000000 ± 0.774597 in #4
Epoch #6: 5001it [03:39, 22.75it/s, env_step=30000, len=50, loss/actor=-7.106, loss/critics=1.296, n/ep=1, n/st=1, rew=-2.00]
Epoch #6: test_reward: -2.000000 ± 1.183216, best_reward: -2.000000 ± 0.774597 in #4
Epoch #7: 5001it [03:42, 22.45it/s, env_step=35000, len=50, loss/actor=-7.395, loss/critics=1.368, n/ep=1, n/st=1, rew=-4.00]
Epoch #7: test_reward: -2.100000 ± 1.044031, best_reward: -2.000000 ± 0.774597 in #4
Epoch #8: 5001it [03:46, 22.11it/s, env_step=40000, len=50, loss/actor=-7.617, loss/critics=1.402, n/ep=1, n/st=1, rew=-11.00]
Epoch #8: test_reward: -2.000000 ± 1.183216, best_reward: -2.000000 ± 0.774597 in #4
Epoch #9: 5001it [03:50, 21.71it/s, env_step=45000, len=50, loss/actor=-7.711, loss/critics=1.433, n/ep=1, n/st=1, rew=-8.00]
Epoch #9: test_reward: -2.300000 ± 0.781025, best_reward: -2.000000 ± 0.774597 in #4
Epoch #10: 5001it [03:50, 21.73it/s, env_step=50000, len=50, loss/actor=-7.841, loss/critics=1.440, n/ep=1, n/st=1, rew=-6.00]
Epoch #10: test_reward: -2.300000 ± 0.900000, best_reward: -2.000000 ± 0.774597 in #4
{'best_result': '-2.00 ± 0.77',
 'best_reward': -2.0,
 'duration': '2162.18s',
 'test_episode': 110,
 'test_speed': '334.27 step/s',
 'test_step': 5500,
 'test_time': '16.45s',
 'train_episode': 1000,
 'train_speed': '23.30 step/s',
 'train_step': 50000,
 'train_time/collector': '432.37s',
 'train_time/model': '1713.36s'}
Final reward: -2.4, length: 50.0