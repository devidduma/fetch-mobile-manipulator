{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IO0ADjYzh7T1",
    "outputId": "d9819c21-998a-475e-feef-d9a3563f6435"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfuzeq7BitX9",
    "outputId": "c84eb126-047d-481c-f2a8-dab3d3bff220"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Markus28/tianshou.git@gymnasium_integration#egg=tianshou --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyOq9We-I6ay",
    "outputId": "f920a588-f536-4c71-b993-ca537e549a73"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "libglew-dev is already the newest version (2.1.0-4).\n",
      "libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "libgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "libosmesa6-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "software-properties-common is already the newest version (0.99.9.10).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-510\n",
      "Use 'apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "patchelf is already the newest version (0.10-2build1).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-510\n",
      "Use 'apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get -q install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get -q install -y patchelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UbOlVWv0157i"
   },
   "outputs": [],
   "source": [
    "!pip install envpool wandb pettingzoo gymnasium-robotics --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fCkkosakI_bC"
   },
   "outputs": [],
   "source": [
    "!pip install free-mujoco-py --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7TZHDrP1JTUj"
   },
   "outputs": [],
   "source": [
    "import mujoco_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jtbz6g5yj3FL",
    "outputId": "66b8d95c-c064-40a2-a212-5991fc7d60c4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2023-02-23 14:28:51.944812: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin\n",
      "2023-02-23 14:28:51.945131: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin\n",
      "2023-02-23 14:28:51.945167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Observations shape: {'observation': (25,), 'achieved_goal': (3,), 'desired_goal': (3,)}\n",
      "Actions shape: (4,)\n",
      "Action range: -1.0 1.0\n",
      "Epoch #1: 5001it [08:29,  9.81it/s, env_step=5000, len=50, loss/actor=0.928, loss/critic=0.265, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #1: test_reward: -50.000000 ± 0.000000, best_reward: -50.000000 ± 0.000000 in #0\n",
      "Epoch #2: 5001it [08:36,  9.69it/s, env_step=10000, len=50, loss/actor=0.460, loss/critic=0.550, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #2: test_reward: -50.000000 ± 0.000000, best_reward: -50.000000 ± 0.000000 in #0\n",
      "Epoch #3: 5001it [08:42,  9.58it/s, env_step=15000, len=50, loss/actor=-1.051, loss/critic=0.466, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #3: test_reward: -50.000000 ± 0.000000, best_reward: -50.000000 ± 0.000000 in #0\n",
      "Epoch #4: 5001it [08:54,  9.35it/s, env_step=20000, len=50, loss/actor=-1.331, loss/critic=0.498, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #4: test_reward: -45.000000 ± 15.000000, best_reward: -45.000000 ± 15.000000 in #4\n",
      "Epoch #5: 5001it [08:52,  9.39it/s, env_step=25000, len=50, loss/actor=-1.216, loss/critic=0.565, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #5: test_reward: -40.000000 ± 20.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #6: 5001it [09:04,  9.18it/s, env_step=30000, len=50, loss/actor=-1.574, loss/critic=0.765, n/ep=1, n/st=1, rew=0.00]\n",
      "Epoch #6: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #7: 5001it [09:00,  9.25it/s, env_step=35000, len=50, loss/actor=-2.389, loss/critic=0.968, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #7: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #8: 5001it [09:10,  9.08it/s, env_step=40000, len=50, loss/actor=-4.591, loss/critic=1.463, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #8: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #9: 5001it [09:14,  9.02it/s, env_step=45000, len=50, loss/actor=-5.588, loss/critic=2.175, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #9: test_reward: -45.000000 ± 15.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #10: 5001it [09:14,  9.03it/s, env_step=50000, len=50, loss/actor=-7.933, loss/critic=3.361, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #10: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #11: 5001it [09:24,  8.86it/s, env_step=55000, len=50, loss/actor=-9.616, loss/critic=3.624, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #11: test_reward: -45.000000 ± 15.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #12: 5001it [09:28,  8.80it/s, env_step=60000, len=50, loss/actor=-10.321, loss/critic=4.757, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #12: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #13: 5001it [09:33,  8.73it/s, env_step=65000, len=50, loss/actor=-14.849, loss/critic=7.144, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #13: test_reward: -49.800000 ± 0.600000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #14: 5001it [09:37,  8.66it/s, env_step=70000, len=50, loss/actor=-17.005, loss/critic=8.443, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #14: test_reward: -47.900000 ± 6.300000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #15: 5001it [09:38,  8.64it/s, env_step=75000, len=50, loss/actor=-19.110, loss/critic=11.339, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #15: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #16: 5001it [09:35,  8.69it/s, env_step=80000, len=50, loss/actor=-22.462, loss/critic=16.559, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #16: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #17: 5001it [09:42,  8.58it/s, env_step=85000, len=50, loss/actor=-26.409, loss/critic=20.625, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #17: test_reward: -45.000000 ± 15.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #18: 5001it [09:37,  8.67it/s, env_step=90000, len=50, loss/actor=-27.156, loss/critic=24.508, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #18: test_reward: -49.100000 ± 2.700000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #19: 5001it [09:38,  8.65it/s, env_step=95000, len=50, loss/actor=-26.372, loss/critic=18.925, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #19: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #20: 5001it [09:40,  8.61it/s, env_step=100000, len=50, loss/actor=-24.810, loss/critic=17.932, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #20: test_reward: -45.000000 ± 15.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #21: 5001it [09:39,  8.62it/s, env_step=105000, len=50, loss/actor=-23.480, loss/critic=14.842, n/ep=1, n/st=1, rew=-46.00]              \n",
      "Epoch #21: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #22: 5001it [09:39,  8.63it/s, env_step=110000, len=50, loss/actor=-23.674, loss/critic=16.438, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #22: test_reward: -45.000000 ± 15.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #23: 5001it [09:41,  8.60it/s, env_step=115000, len=50, loss/actor=-23.613, loss/critic=17.503, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #23: test_reward: -50.000000 ± 0.000000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #24: 5001it [09:40,  8.62it/s, env_step=120000, len=50, loss/actor=-23.639, loss/critic=15.920, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #24: test_reward: -49.500000 ± 1.500000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #25: 5001it [09:39,  8.62it/s, env_step=125000, len=50, loss/actor=-26.203, loss/critic=25.258, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #25: test_reward: -44.900000 ± 12.589281, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #26: 5001it [09:43,  8.56it/s, env_step=130000, len=50, loss/actor=-29.181, loss/critic=28.130, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #26: test_reward: -49.600000 ± 1.200000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #27: 5001it [09:42,  8.58it/s, env_step=135000, len=50, loss/actor=-30.789, loss/critic=28.225, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #27: test_reward: -49.600000 ± 1.200000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #28: 5001it [09:38,  8.65it/s, env_step=140000, len=50, loss/actor=-33.571, loss/critic=29.389, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #28: test_reward: -49.500000 ± 1.500000, best_reward: -40.000000 ± 20.000000 in #5\n",
      "Epoch #29: 5001it [09:38,  8.65it/s, env_step=145000, len=50, loss/actor=-38.945, loss/critic=42.833, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #29: test_reward: -33.500000 ± 22.069209, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #30: 5001it [09:35,  8.68it/s, env_step=150000, len=50, loss/actor=-44.862, loss/critic=56.767, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #30: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #31: 5001it [09:38,  8.64it/s, env_step=155000, len=50, loss/actor=-53.223, loss/critic=72.913, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #31: test_reward: -44.600000 ± 11.993331, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #32: 5001it [09:42,  8.59it/s, env_step=160000, len=50, loss/actor=-63.055, loss/critic=97.022, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #32: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #33: 5001it [09:38,  8.64it/s, env_step=165000, len=50, loss/actor=-68.774, loss/critic=111.401, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #33: test_reward: -46.300000 ± 11.100000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #34: 5001it [09:44,  8.55it/s, env_step=170000, len=50, loss/actor=-69.904, loss/critic=110.747, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #34: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #35: 5001it [09:38,  8.64it/s, env_step=175000, len=50, loss/actor=-69.165, loss/critic=98.957, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #35: test_reward: -45.000000 ± 15.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #36: 5001it [09:45,  8.54it/s, env_step=180000, len=50, loss/actor=-71.018, loss/critic=110.553, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #36: test_reward: -48.900000 ± 2.211334, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #37: 5001it [09:45,  8.54it/s, env_step=185000, len=50, loss/actor=-73.391, loss/critic=115.134, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #37: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #38: 5001it [09:46,  8.53it/s, env_step=190000, len=50, loss/actor=-75.300, loss/critic=125.572, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #38: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #39: 5001it [09:48,  8.50it/s, env_step=195000, len=50, loss/actor=-75.263, loss/critic=124.621, n/ep=1, n/st=1, rew=-49.00]              \n",
      "Epoch #39: test_reward: -43.900000 ± 14.835431, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #40: 5001it [09:48,  8.50it/s, env_step=200000, len=50, loss/actor=-75.441, loss/critic=119.536, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #40: test_reward: -49.600000 ± 0.663325, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #41: 5001it [09:44,  8.55it/s, env_step=205000, len=50, loss/actor=-76.487, loss/critic=126.798, n/ep=1, n/st=1, rew=-48.00]              \n",
      "Epoch #41: test_reward: -45.400000 ± 12.224565, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #42: 5001it [09:46,  8.53it/s, env_step=210000, len=50, loss/actor=-75.793, loss/critic=113.767, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #42: test_reward: -44.900000 ± 14.645477, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #43: 5001it [09:47,  8.51it/s, env_step=215000, len=50, loss/actor=-74.909, loss/critic=122.728, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #43: test_reward: -45.600000 ± 13.200000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #44: 5001it [09:42,  8.59it/s, env_step=220000, len=50, loss/actor=-74.409, loss/critic=117.872, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #44: test_reward: -49.800000 ± 0.600000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #45: 5001it [09:48,  8.50it/s, env_step=225000, len=50, loss/actor=-75.801, loss/critic=112.067, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #45: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #46: 5001it [09:44,  8.56it/s, env_step=230000, len=50, loss/actor=-75.461, loss/critic=116.247, n/ep=1, n/st=1, rew=-48.00]              \n",
      "Epoch #46: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #47: 5001it [09:44,  8.56it/s, env_step=235000, len=50, loss/actor=-73.844, loss/critic=113.523, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #47: test_reward: -49.700000 ± 0.900000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #48: 5001it [09:45,  8.54it/s, env_step=240000, len=50, loss/actor=-70.965, loss/critic=103.458, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #48: test_reward: -46.200000 ± 11.400000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #49: 5001it [09:45,  8.54it/s, env_step=245000, len=50, loss/actor=-69.685, loss/critic=105.162, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #49: test_reward: -44.500000 ± 14.575665, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #50: 5001it [09:43,  8.57it/s, env_step=250000, len=50, loss/actor=-70.352, loss/critic=110.162, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #50: test_reward: -49.700000 ± 0.900000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #51: 5001it [09:47,  8.52it/s, env_step=255000, len=50, loss/actor=-73.578, loss/critic=127.287, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #51: test_reward: -49.500000 ± 1.500000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #52: 5001it [09:43,  8.57it/s, env_step=260000, len=50, loss/actor=-79.039, loss/critic=147.925, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #52: test_reward: -49.400000 ± 1.800000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #53: 5001it [09:44,  8.56it/s, env_step=265000, len=50, loss/actor=-82.778, loss/critic=148.604, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #53: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #54: 5001it [09:45,  8.54it/s, env_step=270000, len=50, loss/actor=-87.797, loss/critic=171.045, n/ep=1, n/st=1, rew=-9.00]              \n",
      "Epoch #54: test_reward: -43.600000 ± 14.698299, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #55: 5001it [09:47,  8.52it/s, env_step=275000, len=50, loss/actor=-94.665, loss/critic=200.784, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #55: test_reward: -49.500000 ± 1.024695, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #56: 5001it [09:46,  8.53it/s, env_step=280000, len=50, loss/actor=-99.898, loss/critic=224.870, n/ep=1, n/st=1, rew=-41.00]              \n",
      "Epoch #56: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #57: 5001it [09:48,  8.50it/s, env_step=285000, len=50, loss/actor=-107.447, loss/critic=258.795, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #57: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #58: 5001it [09:47,  8.52it/s, env_step=290000, len=50, loss/actor=-114.187, loss/critic=307.908, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #58: test_reward: -48.900000 ± 2.211334, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #59: 5001it [09:43,  8.56it/s, env_step=295000, len=50, loss/actor=-119.887, loss/critic=304.572, n/ep=1, n/st=1, rew=-45.00]              \n",
      "Epoch #59: test_reward: -49.900000 ± 0.300000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #60: 5001it [09:47,  8.52it/s, env_step=300000, len=50, loss/actor=-123.456, loss/critic=319.856, n/ep=1, n/st=1, rew=0.00]\n",
      "Epoch #60: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #61: 5001it [09:42,  8.59it/s, env_step=305000, len=50, loss/actor=-126.998, loss/critic=351.613, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #61: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #62: 5001it [09:45,  8.54it/s, env_step=310000, len=50, loss/actor=-131.759, loss/critic=376.624, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #62: test_reward: -49.700000 ± 0.900000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #63: 5001it [09:45,  8.54it/s, env_step=315000, len=50, loss/actor=-139.627, loss/critic=398.587, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #63: test_reward: -46.200000 ± 11.400000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #64: 5001it [09:40,  8.61it/s, env_step=320000, len=50, loss/actor=-146.833, loss/critic=484.738, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #64: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #65: 5001it [09:46,  8.53it/s, env_step=325000, len=50, loss/actor=-155.281, loss/critic=516.568, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #65: test_reward: -49.800000 ± 0.600000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #66: 5001it [09:45,  8.54it/s, env_step=330000, len=50, loss/actor=-161.785, loss/critic=558.619, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #66: test_reward: -49.800000 ± 0.600000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #67: 5001it [09:45,  8.54it/s, env_step=335000, len=50, loss/actor=-167.680, loss/critic=626.027, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #67: test_reward: -48.600000 ± 3.583295, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #68: 5001it [09:43,  8.57it/s, env_step=340000, len=50, loss/actor=-172.573, loss/critic=653.470, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #68: test_reward: -49.400000 ± 1.800000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #69: 5001it [09:45,  8.54it/s, env_step=345000, len=50, loss/actor=-180.513, loss/critic=756.215, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #69: test_reward: -41.500000 ± 17.013230, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #70: 5001it [09:40,  8.61it/s, env_step=350000, len=50, loss/actor=-193.000, loss/critic=897.799, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #70: test_reward: -49.600000 ± 1.200000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #71: 5001it [09:47,  8.51it/s, env_step=355000, len=50, loss/actor=-203.402, loss/critic=960.931, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #71: test_reward: -49.000000 ± 2.408319, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #72: 5001it [09:44,  8.56it/s, env_step=360000, len=50, loss/actor=-206.486, loss/critic=945.491, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #72: test_reward: -41.700000 ± 16.673632, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #73: 5001it [09:48,  8.50it/s, env_step=365000, len=50, loss/actor=-205.547, loss/critic=910.022, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #73: test_reward: -44.100000 ± 14.501379, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #74: 5001it [09:47,  8.51it/s, env_step=370000, len=50, loss/actor=-199.727, loss/critic=869.668, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #74: test_reward: -49.600000 ± 1.200000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #75: 5001it [09:47,  8.51it/s, env_step=375000, len=50, loss/actor=-198.049, loss/critic=886.305, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #75: test_reward: -49.200000 ± 2.400000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #76: 5001it [09:44,  8.55it/s, env_step=380000, len=50, loss/actor=-203.505, loss/critic=945.954, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #76: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #77: 5001it [09:47,  8.51it/s, env_step=385000, len=50, loss/actor=-208.518, loss/critic=950.165, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #77: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #78: 5001it [09:41,  8.60it/s, env_step=390000, len=50, loss/actor=-217.723, loss/critic=1048.619, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #78: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #79: 5001it [09:42,  8.58it/s, env_step=395000, len=50, loss/actor=-224.022, loss/critic=1104.967, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #79: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #80: 5001it [09:46,  8.53it/s, env_step=400000, len=50, loss/actor=-233.805, loss/critic=1292.085, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #80: test_reward: -41.200000 ± 17.741477, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #81: 5001it [09:44,  8.55it/s, env_step=405000, len=50, loss/actor=-244.074, loss/critic=1374.979, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #81: test_reward: -49.600000 ± 1.200000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #82: 5001it [09:49,  8.49it/s, env_step=410000, len=50, loss/actor=-261.391, loss/critic=1483.026, n/ep=1, n/st=1, rew=-44.00]              \n",
      "Epoch #82: test_reward: -49.700000 ± 0.900000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #83: 5001it [09:49,  8.49it/s, env_step=415000, len=50, loss/actor=-278.752, loss/critic=1720.525, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #83: test_reward: -45.100000 ± 13.072490, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #84: 5001it [09:46,  8.53it/s, env_step=420000, len=50, loss/actor=-299.466, loss/critic=2117.533, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #84: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #85: 5001it [09:43,  8.57it/s, env_step=425000, len=50, loss/actor=-318.343, loss/critic=2175.107, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #85: test_reward: -49.700000 ± 0.900000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #86: 5001it [09:42,  8.59it/s, env_step=430000, len=50, loss/actor=-334.318, loss/critic=2331.450, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #86: test_reward: -44.800000 ± 14.945233, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #87: 5001it [09:40,  8.62it/s, env_step=435000, len=50, loss/actor=-352.036, loss/critic=2639.591, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #87: test_reward: -49.400000 ± 1.800000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #88: 5001it [09:45,  8.54it/s, env_step=440000, len=50, loss/actor=-363.446, loss/critic=2687.576, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #88: test_reward: -48.700000 ± 2.830194, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #89: 5001it [09:41,  8.60it/s, env_step=445000, len=50, loss/actor=-375.613, loss/critic=3184.658, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #89: test_reward: -49.300000 ± 1.417745, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #90: 5001it [09:43,  8.58it/s, env_step=450000, len=50, loss/actor=-396.900, loss/critic=3349.541, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #90: test_reward: -47.000000 ± 8.671793, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #91: 5001it [09:42,  8.58it/s, env_step=455000, len=50, loss/actor=-426.331, loss/critic=3936.043, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #91: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #92: 5001it [09:44,  8.56it/s, env_step=460000, len=50, loss/actor=-449.879, loss/critic=4197.238, n/ep=1, n/st=1, rew=-47.00]              \n",
      "Epoch #92: test_reward: -49.400000 ± 1.800000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #93: 5001it [09:43,  8.57it/s, env_step=465000, len=50, loss/actor=-472.019, loss/critic=5181.183, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #93: test_reward: -49.600000 ± 1.200000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #94: 5001it [09:46,  8.53it/s, env_step=470000, len=50, loss/actor=-472.363, loss/critic=4953.941, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #94: test_reward: -49.400000 ± 1.200000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #95: 5001it [09:43,  8.58it/s, env_step=475000, len=50, loss/actor=-465.543, loss/critic=4652.354, n/ep=1, n/st=1, rew=-44.00]\n",
      "Epoch #95: test_reward: -48.600000 ± 4.200000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #96: 5001it [09:38,  8.64it/s, env_step=480000, len=50, loss/actor=-464.805, loss/critic=4812.643, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #96: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #97: 5001it [09:43,  8.57it/s, env_step=485000, len=50, loss/actor=-486.185, loss/critic=5179.037, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #97: test_reward: -50.000000 ± 0.000000, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #98: 5001it [09:38,  8.65it/s, env_step=490000, len=50, loss/actor=-512.142, loss/critic=5813.396, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #98: test_reward: -41.000000 ± 18.138357, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #99: 5001it [09:39,  8.64it/s, env_step=495000, len=50, loss/actor=-528.561, loss/critic=6359.994, n/ep=1, n/st=1, rew=-50.00]              \n",
      "Epoch #99: test_reward: -44.500000 ± 14.908052, best_reward: -33.500000 ± 22.069209 in #29\n",
      "Epoch #100: 5001it [09:41,  8.60it/s, env_step=500000, len=50, loss/actor=-529.074, loss/critic=6858.195, n/ep=1, n/st=1, rew=-50.00]\n",
      "Epoch #100: test_reward: -45.200000 ± 8.352245, best_reward: -33.500000 ± 22.069209 in #29\n",
      "{'best_result': '-33.50 ± 22.07',\n",
      " 'best_reward': -33.5,\n",
      " 'duration': '58032.95s',\n",
      " 'test_episode': 1010,\n",
      " 'test_speed': '257.47 step/s',\n",
      " 'test_step': 50500,\n",
      " 'test_time': '196.14s',\n",
      " 'train_episode': 10000,\n",
      " 'train_speed': '8.65 step/s',\n",
      " 'train_step': 500000,\n",
      " 'train_time/collector': '7311.76s',\n",
      " 'train_time/model': '50525.04s'}\n",
      "Final reward: -49.9, length: 50.0\n"
     ]
    }
   ],
   "source": [
    "!python fetch_her_ddpg.py --task FetchPush-v2 \\\n",
    "      --hidden-sizes 256 256 256 --actor-lr 1e-4 --critic-lr 1e-4 \\\n",
    "      --gamma 0.98 --epoch 100 --her-horizon 50"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
