2023-02-12 16:06:42.024510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin
2023-02-12 16:06:42.024653: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin
2023-02-12 16:06:42.024677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Observations shape: {'observation': (10,), 'achieved_goal': (3,), 'desired_goal': (3,)}
Actions shape: (4,)
Action range: -1.0 1.0
Epoch #1: 5001it [06:14, 13.35it/s, env_step=5000, len=50, loss/actor=3.196, loss/critic=0.278, n/ep=1, n/st=1, rew=-2.00]
Epoch #1: test_reward: -1.600000 ± 0.916515, best_reward: -1.600000 ± 0.916515 in #1
Epoch #2: 5001it [06:38, 12.55it/s, env_step=10000, len=50, loss/actor=-0.598, loss/critic=0.145, n/ep=1, n/st=1, rew=-2.00]
Epoch #2: test_reward: -1.800000 ± 0.748331, best_reward: -1.600000 ± 0.916515 in #1
Epoch #3: 5001it [06:54, 12.05it/s, env_step=15000, len=50, loss/actor=-0.276, loss/critic=0.094, n/ep=1, n/st=1, rew=-3.00]
Epoch #3: test_reward: -1.600000 ± 0.916515, best_reward: -1.600000 ± 0.916515 in #1
Epoch #4: 5001it [07:04, 11.77it/s, env_step=20000, len=50, loss/actor=-0.162, loss/critic=0.074, n/ep=1, n/st=1, rew=0.00]
Epoch #4: test_reward: -1.500000 ± 1.024695, best_reward: -1.500000 ± 1.024695 in #4
Epoch #5: 5001it [07:09, 11.65it/s, env_step=25000, len=50, loss/actor=-0.210, loss/critic=0.075, n/ep=1, n/st=1, rew=-4.00]
Epoch #5: test_reward: -1.900000 ± 0.538516, best_reward: -1.500000 ± 1.024695 in #4
Epoch #6: 5001it [07:14, 11.50it/s, env_step=30000, len=50, loss/actor=-0.263, loss/critic=0.068, n/ep=1, n/st=1, rew=-2.00]
Epoch #6: test_reward: -1.200000 ± 0.979796, best_reward: -1.200000 ± 0.979796 in #6
Epoch #7: 5001it [07:23, 11.27it/s, env_step=35000, len=50, loss/actor=-0.147, loss/critic=0.058, n/ep=1, n/st=1, rew=-3.00]
Epoch #7: test_reward: -1.800000 ± 0.600000, best_reward: -1.200000 ± 0.979796 in #6
Epoch #8: 5001it [07:24, 11.26it/s, env_step=40000, len=50, loss/actor=0.016, loss/critic=0.048, n/ep=1, n/st=1, rew=-1.00]
Epoch #8: test_reward: -1.200000 ± 0.871780, best_reward: -1.200000 ± 0.979796 in #6
Epoch #9: 5001it [07:21, 11.33it/s, env_step=45000, len=50, loss/actor=0.032, loss/critic=0.046, n/ep=1, n/st=1, rew=-2.00]
Epoch #9: test_reward: -1.900000 ± 0.700000, best_reward: -1.200000 ± 0.979796 in #6
Epoch #10: 5001it [07:18, 11.40it/s, env_step=50000, len=50, loss/actor=0.066, loss/critic=0.037, n/ep=1, n/st=1, rew=-2.00]
Epoch #10: test_reward: -1.500000 ± 0.921954, best_reward: -1.200000 ± 0.979796 in #6
{'best_result': '-1.20 ± 0.98',
 'best_reward': -1.2,
 'duration': '4264.65s',
 'test_episode': 110,
 'test_speed': '288.84 step/s',
 'test_step': 5500,
 'test_time': '19.04s',
 'train_episode': 1000,
 'train_speed': '11.78 step/s',
 'train_step': 50000,
 'train_time/collector': '616.78s',
 'train_time/model': '3628.83s'}
Final reward: -2.1, length: 50.0