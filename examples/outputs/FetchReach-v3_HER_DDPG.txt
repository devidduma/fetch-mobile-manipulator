G:\DD\fetch-mobile-manipulator\venv\Scripts\python.exe G:\DD\fetch-mobile-manipulator\examples\fetch_her_ddpg.py --task FetchReach-v3 --hidden-sizes 256 256 --actor-lr 1e-3 --critic-lr 3e-3 --gamma 0.98 --epoch 10 --step-per-epoch 5000
Observations shape: {'observation': (10,), 'achieved_goal': (3,), 'desired_goal': (3,)}
Actions shape: (4,)
Action range: -1.0 1.0
Epoch #1: 5001it [02:51, 29.16it/s, env_step=5000, len=50, loss/actor=4.146, loss/critic=0.388, n/ep=1, n/st=1, rew=-5.00]
Epoch #1: test_reward: -5.000000 ± 10.344080, best_reward: -5.000000 ± 10.344080 in #1
Epoch #2: 5001it [02:47, 29.77it/s, env_step=10000, len=50, loss/actor=0.571, loss/critic=0.096, n/ep=1, n/st=1, rew=0.00]
Epoch #2: test_reward: -2.200000 ± 0.748331, best_reward: -2.200000 ± 0.748331 in #2
Epoch #3: 5001it [02:53, 28.84it/s, env_step=15000, len=50, loss/actor=0.272, loss/critic=0.089, n/ep=1, n/st=1, rew=-2.00]
Epoch #3: test_reward: -2.500000 ± 0.500000, best_reward: -2.200000 ± 0.748331 in #2
Epoch #4: 5001it [02:58, 28.09it/s, env_step=20000, len=50, loss/actor=0.196, loss/critic=0.076, n/ep=1, n/st=1, rew=-2.00]
Epoch #4: test_reward: -1.900000 ± 0.538516, best_reward: -1.900000 ± 0.538516 in #4
Epoch #5: 5001it [03:03, 27.24it/s, env_step=25000, len=50, loss/actor=0.258, loss/critic=0.060, n/ep=1, n/st=1, rew=-2.00]
Epoch #6:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #5: test_reward: -1.200000 ± 0.871780, best_reward: -1.200000 ± 0.871780 in #5
Epoch #6: 5001it [03:06, 26.85it/s, env_step=30000, len=50, loss/actor=0.295, loss/critic=0.061, n/ep=1, n/st=1, rew=-2.00]
Epoch #6: test_reward: -1.600000 ± 0.800000, best_reward: -1.200000 ± 0.871780 in #5
Epoch #7: 5001it [03:05, 26.92it/s, env_step=35000, len=50, loss/actor=0.183, loss/critic=0.053, n/ep=1, n/st=1, rew=0.00]
Epoch #8:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #7: test_reward: -1.800000 ± 0.748331, best_reward: -1.200000 ± 0.871780 in #5
Epoch #8: 5001it [03:09, 26.37it/s, env_step=40000, len=50, loss/actor=0.248, loss/critic=0.055, n/ep=1, n/st=1, rew=-2.00]
Epoch #9:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #8: test_reward: -1.800000 ± 0.748331, best_reward: -1.200000 ± 0.871780 in #5
Epoch #9: 5001it [03:10, 26.31it/s, env_step=45000, len=50, loss/actor=0.312, loss/critic=0.047, n/ep=1, n/st=1, rew=-2.00]
Epoch #10:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #9: test_reward: -1.700000 ± 0.900000, best_reward: -1.200000 ± 0.871780 in #5
Epoch #10: 5001it [03:11, 26.08it/s, env_step=50000, len=50, loss/actor=0.296, loss/critic=0.041, n/ep=1, n/st=1, rew=0.00]
Epoch #10: test_reward: -1.900000 ± 0.538516, best_reward: -1.200000 ± 0.871780 in #5
{'best_result': '-1.20 ± 0.87',
 'best_reward': -1.2,
 'duration': '1821.17s',
 'test_episode': 110,
 'test_speed': '1778.20 step/s',
 'test_step': 5500,
 'test_time': '3.09s',
 'train_episode': 1000,
 'train_speed': '27.50 step/s',
 'train_step': 50000,
 'train_time/collector': '364.81s',
 'train_time/model': '1453.26s'}
Final reward: -2.2, length: 50.0

Process finished with exit code 0
