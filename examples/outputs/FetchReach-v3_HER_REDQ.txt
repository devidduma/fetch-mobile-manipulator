G:\DD\fetch-mobile-manipulator\venv\Scripts\python.exe G:\DD\fetch-mobile-manipulator\examples\fetch_her_redq.py --task FetchReach-v3 --hidden-sizes 256 256 --actor-lr 1e-3 --critic-lr 3e-3 --gamma 0.98 --epoch 10 --step-per-epoch 5000
Observations shape: {'observation': (10,), 'achieved_goal': (3,), 'desired_goal': (3,)}
Actions shape: (4,)
Action range: -1.0 1.0
Epoch #1: 5001it [03:15, 25.60it/s, env_step=5000, len=50, loss/actor=-0.504, loss/critics=0.275, n/ep=1, n/st=1, rew=-3.00]
Epoch #2:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #1: test_reward: -1.700000 ± 0.781025, best_reward: -1.700000 ± 0.781025 in #1
Epoch #2: 5001it [03:09, 26.34it/s, env_step=10000, len=50, loss/actor=-2.746, loss/critics=0.419, n/ep=1, n/st=1, rew=-4.00]
Epoch #2: test_reward: -1.800000 ± 1.077033, best_reward: -1.700000 ± 0.781025 in #1
Epoch #3: 5001it [03:20, 24.89it/s, env_step=15000, len=50, loss/actor=-3.933, loss/critics=0.617, n/ep=1, n/st=1, rew=-7.00]
Epoch #4:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #3: test_reward: -1.500000 ± 1.204159, best_reward: -1.500000 ± 1.204159 in #3
Epoch #4: 5001it [03:28, 24.01it/s, env_step=20000, len=50, loss/actor=-4.743, loss/critics=0.688, n/ep=1, n/st=1, rew=-5.00]
Epoch #5:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #4: test_reward: -1.900000 ± 1.135782, best_reward: -1.500000 ± 1.204159 in #3
Epoch #5: 5001it [03:35, 23.25it/s, env_step=25000, len=50, loss/actor=-5.261, loss/critics=0.853, n/ep=1, n/st=1, rew=-7.00]
Epoch #5: test_reward: -1.800000 ± 1.249000, best_reward: -1.500000 ± 1.204159 in #3
Epoch #6: 5001it [03:39, 22.82it/s, env_step=30000, len=50, loss/actor=-5.614, loss/critics=0.877, n/ep=1, n/st=1, rew=-7.00]
Epoch #6: test_reward: -2.100000 ± 0.700000, best_reward: -1.500000 ± 1.204159 in #3
Epoch #7: 5001it [03:39, 22.74it/s, env_step=35000, len=50, loss/actor=-5.819, loss/critics=0.900, n/ep=1, n/st=1, rew=-5.00]
Epoch #8:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #7: test_reward: -2.800000 ± 0.748331, best_reward: -1.500000 ± 1.204159 in #3
Epoch #8: 5001it [03:41, 22.54it/s, env_step=40000, len=50, loss/actor=-5.909, loss/critics=0.895, n/ep=1, n/st=1, rew=-6.00]
Epoch #8: test_reward: -2.100000 ± 1.044031, best_reward: -1.500000 ± 1.204159 in #3
Epoch #9: 5001it [03:39, 22.79it/s, env_step=45000, len=50, loss/actor=-5.916, loss/critics=0.906, n/ep=1, n/st=1, rew=-7.00]
Epoch #10:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #9: test_reward: -1.700000 ± 1.100000, best_reward: -1.500000 ± 1.204159 in #3
Epoch #10: 5001it [03:26, 24.16it/s, env_step=50000, len=50, loss/actor=-6.051, loss/critics=0.920, n/ep=1, n/st=1, rew=-8.00]
Epoch #10: test_reward: -2.200000 ± 0.979796, best_reward: -1.500000 ± 1.204159 in #3
{'best_result': '-1.50 ± 1.20',
 'best_reward': -1.5,
 'duration': '2100.62s',
 'test_episode': 110,
 'test_speed': '1471.75 step/s',
 'test_step': 5500,
 'test_time': '3.74s',
 'train_episode': 1000,
 'train_speed': '23.84 step/s',
 'train_step': 50000,
 'train_time/collector': '417.84s',
 'train_time/model': '1679.05s'}
Final reward: -2.4, length: 50.0

Process finished with exit code 0
