2023-02-12 16:09:48.893856: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin
2023-02-12 16:09:48.894036: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/bin
2023-02-12 16:09:48.894066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Observations shape: {'observation': (10,), 'achieved_goal': (3,), 'desired_goal': (3,)}
Actions shape: (4,)
Action range: -1.0 1.0
Epoch #1: 5001it [23:28,  3.55it/s, env_step=5000, len=50, loss/actor=-0.589, loss/critics=0.278, n/ep=1, n/st=1, rew=-3.00]
Epoch #1: test_reward: -1.800000 ± 1.166190, best_reward: -1.800000 ± 1.166190 in #1
Epoch #2: 5001it [24:25,  3.41it/s, env_step=10000, len=50, loss/actor=-3.472, loss/critics=0.520, n/ep=1, n/st=1, rew=-4.00]
Epoch #2: test_reward: -2.600000 ± 0.916515, best_reward: -1.800000 ± 1.166190 in #1
Epoch #3: 5001it [23:07,  3.60it/s, env_step=15000, len=50, loss/actor=-4.969, loss/critics=0.835, n/ep=1, n/st=1, rew=-3.00]
Epoch #3: test_reward: -2.200000 ± 0.871780, best_reward: -1.800000 ± 1.166190 in #1
Epoch #4: 5001it [24:12,  3.44it/s, env_step=20000, len=50, loss/actor=-5.787, loss/critics=0.900, n/ep=1, n/st=1, rew=-7.00]
Epoch #4: test_reward: -1.700000 ± 1.004988, best_reward: -1.700000 ± 1.004988 in #4
Epoch #5: 5001it [24:17,  3.43it/s, env_step=25000, len=50, loss/actor=-6.309, loss/critics=1.117, n/ep=1, n/st=1, rew=-4.00]
Epoch #5: test_reward: -2.200000 ± 0.979796, best_reward: -1.700000 ± 1.004988 in #4
Epoch #6: 5001it [24:38,  3.38it/s, env_step=30000, len=50, loss/actor=-6.645, loss/critics=1.120, n/ep=1, n/st=1, rew=-9.00]
Epoch #6: test_reward: -2.000000 ± 1.000000, best_reward: -1.700000 ± 1.004988 in #4
Epoch #7: 5001it [25:02,  3.33it/s, env_step=35000, len=50, loss/actor=-6.964, loss/critics=1.177, n/ep=1, n/st=1, rew=-1.00]
Epoch #7: test_reward: -2.200000 ± 0.748331, best_reward: -1.700000 ± 1.004988 in #4
Epoch #8: 5001it [24:39,  3.38it/s, env_step=40000, len=50, loss/actor=-7.192, loss/critics=1.232, n/ep=1, n/st=1, rew=-13.00]
Epoch #8: test_reward: -2.500000 ± 1.204159, best_reward: -1.700000 ± 1.004988 in #4
Epoch #9: 5001it [24:48,  3.36it/s, env_step=45000, len=50, loss/actor=-7.320, loss/critics=1.267, n/ep=1, n/st=1, rew=-8.00]
Epoch #9: test_reward: -2.100000 ± 1.044031, best_reward: -1.700000 ± 1.004988 in #4
Epoch #10: 5001it [25:09,  3.31it/s, env_step=50000, len=50, loss/actor=-7.551, loss/critics=1.309, n/ep=1, n/st=1, rew=-8.00]
Epoch #10: test_reward: -2.500000 ± 0.500000, best_reward: -1.700000 ± 1.004988 in #4
{'best_result': '-1.70 ± 1.00',
 'best_reward': -1.7,
 'duration': '14651.73s',
 'test_episode': 110,
 'test_speed': '251.17 step/s',
 'test_step': 5500,
 'test_time': '21.90s',
 'train_episode': 1000,
 'train_speed': '3.42 step/s',
 'train_step': 50000,
 'train_time/collector': '851.97s',
 'train_time/model': '13777.86s'}
Final reward: -2.4, length: 50.0