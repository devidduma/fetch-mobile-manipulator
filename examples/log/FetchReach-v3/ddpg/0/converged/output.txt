C:\Users\devid\PycharmProjects\thesis\venv\Scripts\python.exe C:\Users\devid\PycharmProjects\thesis\examples\fetch_her_ddpg.py
Observations shape: {'observation': (10,), 'achieved_goal': (3,), 'desired_goal': (3,)}
Actions shape: (4,)
Action range: -1.0 1.0
Epoch #1: 5001it [03:12, 26.01it/s, env_step=5000, len=50, loss/actor=9.708, loss/critic=1.560, n/ep=1, n/st=1, rew=-46.00]
Epoch #2:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #1: test_reward: -7.100000 ± 14.446107, best_reward: -7.100000 ± 14.446107 in #1
Epoch #2: 5001it [02:22, 34.98it/s, env_step=10000, len=50, loss/actor=1.404, loss/critic=0.094, n/ep=1, n/st=1, rew=-2.00]
Epoch #2: test_reward: -1.500000 ± 1.118034, best_reward: -1.500000 ± 1.118034 in #2
Epoch #3: 5001it [02:26, 34.15it/s, env_step=15000, len=50, loss/actor=0.626, loss/critic=0.080, n/ep=1, n/st=1, rew=-2.00]
Epoch #3: test_reward: -1.100000 ± 0.830662, best_reward: -1.100000 ± 0.830662 in #3
Epoch #4: 5001it [04:33, 18.26it/s, env_step=20000, len=50, loss/actor=0.424, loss/critic=0.067, n/ep=1, n/st=1, rew=-3.00]
Epoch #5:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #4: test_reward: -1.600000 ± 0.916515, best_reward: -1.100000 ± 0.830662 in #3
Epoch #5: 5001it [06:39, 12.53it/s, env_step=25000, len=50, loss/actor=0.415, loss/critic=0.059, n/ep=1, n/st=1, rew=0.00]
Epoch #5: test_reward: -1.600000 ± 1.200000, best_reward: -1.100000 ± 0.830662 in #3
Epoch #6: 5001it [03:15, 25.57it/s, env_step=30000, len=50, loss/actor=0.391, loss/critic=0.054, n/ep=1, n/st=1, rew=-3.00]
Epoch #6: test_reward: -1.500000 ± 1.118034, best_reward: -1.100000 ± 0.830662 in #3
Epoch #7: 5001it [02:35, 32.14it/s, env_step=35000, len=50, loss/actor=0.395, loss/critic=0.060, n/ep=1, n/st=1, rew=-2.00]
Epoch #7: test_reward: -1.700000 ± 0.458258, best_reward: -1.100000 ± 0.830662 in #3
Epoch #8: 5001it [02:37, 31.77it/s, env_step=40000, len=50, loss/actor=-0.141, loss/critic=0.060, n/ep=1, n/st=1, rew=-3.00]
Epoch #9:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #8: test_reward: -1.100000 ± 0.700000, best_reward: -1.100000 ± 0.830662 in #3
Epoch #9: 5001it [02:40, 31.24it/s, env_step=45000, len=50, loss/actor=-0.613, loss/critic=0.073, n/ep=1, n/st=1, rew=-3.00]
Epoch #10:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #9: test_reward: -1.600000 ± 1.019804, best_reward: -1.100000 ± 0.830662 in #3
Epoch #10: 5001it [02:42, 30.82it/s, env_step=50000, len=50, loss/actor=-1.168, loss/critic=0.096, n/ep=1, n/st=1, rew=-3.00]
Epoch #10: test_reward: -1.500000 ± 0.921954, best_reward: -1.100000 ± 0.830662 in #3
{'best_result': '-1.10 ± 0.83',
 'best_reward': -1.1,
 'duration': '1988.16s',
 'test_episode': 110,
 'test_speed': '2244.37 step/s',
 'test_step': 5500,
 'test_time': '2.45s',
 'train_episode': 1000,
 'train_speed': '25.18 step/s',
 'train_step': 50000,
 'train_time/collector': '262.68s',
 'train_time/model': '1723.03s'}
Final reward: -2.2, length: 50.0
