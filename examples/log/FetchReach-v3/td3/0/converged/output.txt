C:\Users\devid\PycharmProjects\thesis\venv\Scripts\python.exe C:\Users\devid\PycharmProjects\thesis\examples\fetch_her_td3.py
Observations shape: {'observation': (10,), 'achieved_goal': (3,), 'desired_goal': (3,)}
Actions shape: (4,)
Action range: -1.0 1.0
Epoch #1: 5001it [04:14, 19.64it/s, env_step=5000, len=50, loss/actor=2.289, loss/critic1=0.238, loss/critic2=0.235, n/ep=1, n/st=1, rew=0.00]
Epoch #1: test_reward: -1.500000 ± 1.204159, best_reward: -1.500000 ± 1.204159 in #1
Epoch #2: 5001it [03:39, 22.83it/s, env_step=10000, len=50, loss/actor=0.992, loss/critic1=0.110, loss/critic2=0.114, n/ep=1, n/st=1, rew=-2.00]
Epoch #3:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #2: test_reward: -1.400000 ± 0.800000, best_reward: -1.400000 ± 0.800000 in #2
Epoch #3: 5001it [02:41, 30.97it/s, env_step=15000, len=50, loss/actor=0.671, loss/critic1=0.083, loss/critic2=0.088, n/ep=1, n/st=1, rew=-2.00]
Epoch #4:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #3: test_reward: -1.600000 ± 0.800000, best_reward: -1.400000 ± 0.800000 in #2
Epoch #4: 5001it [06:21, 13.12it/s, env_step=20000, len=50, loss/actor=0.489, loss/critic1=0.069, loss/critic2=0.070, n/ep=1, n/st=1, rew=-2.00]
Epoch #5:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #4: test_reward: -2.100000 ± 0.700000, best_reward: -1.400000 ± 0.800000 in #2
Epoch #5: 5001it [03:24, 24.50it/s, env_step=25000, len=50, loss/actor=0.414, loss/critic1=0.066, loss/critic2=0.066, n/ep=1, n/st=1, rew=-3.00]
Epoch #5: test_reward: -1.500000 ± 0.806226, best_reward: -1.400000 ± 0.800000 in #2
Epoch #6: 5001it [04:04, 20.48it/s, env_step=30000, len=50, loss/actor=0.401, loss/critic1=0.061, loss/critic2=0.059, n/ep=1, n/st=1, rew=-1.00]
Epoch #6: test_reward: -1.600000 ± 0.916515, best_reward: -1.400000 ± 0.800000 in #2
Epoch #7: 5001it [12:41:16,  9.13s/it, env_step=35000, len=50, loss/actor=0.358, loss/critic1=0.057, loss/critic2=0.057, n/ep=1, n/st=1, rew=-2.00]
Epoch #8:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #7: test_reward: -1.300000 ± 1.004988, best_reward: -1.300000 ± 1.004988 in #7
Epoch #8: 5001it [03:10, 26.29it/s, env_step=40000, len=50, loss/actor=0.339, loss/critic1=0.050, loss/critic2=0.048, n/ep=1, n/st=1, rew=-1.00]
Epoch #9:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #8: test_reward: -1.800000 ± 1.326650, best_reward: -1.300000 ± 1.004988 in #7
Epoch #9: 5001it [04:52, 17.12it/s, env_step=45000, len=50, loss/actor=0.301, loss/critic1=0.052, loss/critic2=0.048, n/ep=1, n/st=1, rew=-1.00]
Epoch #10:   0%|          | 0/5000 [00:00<?, ?it/s]Epoch #9: test_reward: -1.900000 ± 0.830662, best_reward: -1.300000 ± 1.004988 in #7
Epoch #10: 5001it [04:47, 17.39it/s, env_step=50000, len=50, loss/actor=0.290, loss/critic1=0.037, loss/critic2=0.035, n/ep=1, n/st=1, rew=0.00]
Epoch #10: test_reward: -1.500000 ± 0.806226, best_reward: -1.300000 ± 1.004988 in #7
{'best_result': '-1.30 ± 1.00',
 'best_reward': -1.3,
 'duration': '47917.63s',
 'test_episode': 110,
 'test_speed': '911.78 step/s',
 'test_step': 5500,
 'test_time': '6.03s',
 'train_episode': 1000,
 'train_speed': '1.04 step/s',
 'train_step': 50000,
 'train_time/collector': '1952.16s',
 'train_time/model': '45959.44s'}
Final reward: -2.1, length: 50.0

Process finished with exit code 0
